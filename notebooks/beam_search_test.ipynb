{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iIBx-hIdoxwC"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/7embl4/asr.git asr\n",
        "%cd asr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irUDs-fcpCq0",
        "outputId": "d91dc71b-9f72-4acb-fb46-e374c1a3a7cd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'asr'...\n",
            "remote: Enumerating objects: 366, done.\u001b[K\n",
            "remote: Counting objects: 100% (366/366), done.\u001b[K\n",
            "remote: Compressing objects: 100% (222/222), done.\u001b[K\n",
            "remote: Total 366 (delta 185), reused 297 (delta 116), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (366/366), 383.82 KiB | 1.94 MiB/s, done.\n",
            "Resolving deltas: 100% (185/185), done.\n",
            "/content/asr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "aqxEx6m5pEVM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli download artem1085715/conformer-small --local-dir 'model/'\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "J7EmDoA1pIXi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python inference.py \\\n",
        "        inferencer.from_pretrained=\"model/model_best.pth\" \\\n",
        "        metrics.inference.2.type=\"handcrafted\" \\\n",
        "        metrics.inference.2.beam_size=4 \\\n",
        "        metrics.inference.3.type=\"handcrafted\" \\\n",
        "        metrics.inference.3.beam_size=4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZuzgJlwpI_Z",
        "outputId": "91d7932c-fca9-451f-f6bc-5218c87c7eb9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading part test-clean\n",
            "Preparing librispeech folders: test-clean: 100% 87/87 [00:07<00:00, 11.11it/s]\n",
            "Loading part test-other\n",
            "Preparing librispeech folders: test-other: 100% 90/90 [00:07<00:00, 11.48it/s]\n",
            "CTCModel(\n",
            "  (subsampling): Subsampling(\n",
            "    (conv): Conv1d(80, 80, kernel_size=(2,), stride=(2,))\n",
            "    (linear): Linear(in_features=80, out_features=144, bias=True)\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (conf_blocks): Sequential(\n",
            "    (0): ConformerBlock(\n",
            "      (layers): Sequential(\n",
            "        (0): FeedForwardModule(\n",
            "          (layers): Sequential(\n",
            "            (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "            (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "            (2): SiLU()\n",
            "            (3): Dropout(p=0.1, inplace=False)\n",
            "            (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "            (5): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): MultiHeadAttentionModule(\n",
            "          (layers): Sequential(\n",
            "            (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "            (1): TransformerEncoder(\n",
            "              (layers): ModuleList(\n",
            "                (0-15): 16 x TransformerEncoderLayer(\n",
            "                  (self_attn): MultiheadAttention(\n",
            "                    (out_proj): NonDynamicallyQuantizableLinear(in_features=144, out_features=144, bias=True)\n",
            "                  )\n",
            "                  (linear1): Linear(in_features=144, out_features=256, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  (linear2): Linear(in_features=256, out_features=144, bias=True)\n",
            "                  (norm1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "                  (norm2): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "                  (dropout1): Dropout(p=0.1, inplace=False)\n",
            "                  (dropout2): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (2): ConvolutionModule(\n",
            "          (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (layers): Sequential(\n",
            "            (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "            (1): GLU(dim=1)\n",
            "            (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "            (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (4): SiLU()\n",
            "            (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "            (6): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): FeedForwardModule(\n",
            "          (layers): Sequential(\n",
            "            (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "            (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "            (2): SiLU()\n",
            "            (3): Dropout(p=0.1, inplace=False)\n",
            "            (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "            (5): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (1): ConformerBlock(\n",
            "      (layers): Sequential(\n",
            "        (0): FeedForwardModule(\n",
            "          (layers): Sequential(\n",
            "            (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "            (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "            (2): SiLU()\n",
            "            (3): Dropout(p=0.1, inplace=False)\n",
            "            (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "            (5): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): MultiHeadAttentionModule(\n",
            "          (layers): Sequential(\n",
            "            (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "            (1): TransformerEncoder(\n",
            "              (layers): ModuleList(\n",
            "                (0-15): 16 x TransformerEncoderLayer(\n",
            "                  (self_attn): MultiheadAttention(\n",
            "                    (out_proj): NonDynamicallyQuantizableLinear(in_features=144, out_features=144, bias=True)\n",
            "                  )\n",
            "                  (linear1): Linear(in_features=144, out_features=256, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  (linear2): Linear(in_features=256, out_features=144, bias=True)\n",
            "                  (norm1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "                  (norm2): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "                  (dropout1): Dropout(p=0.1, inplace=False)\n",
            "                  (dropout2): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (2): ConvolutionModule(\n",
            "          (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (layers): Sequential(\n",
            "            (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "            (1): GLU(dim=1)\n",
            "            (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "            (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (4): SiLU()\n",
            "            (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "            (6): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): FeedForwardModule(\n",
            "          (layers): Sequential(\n",
            "            (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "            (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "            (2): SiLU()\n",
            "            (3): Dropout(p=0.1, inplace=False)\n",
            "            (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "            (5): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (2): ConformerBlock(\n",
            "      (layers): Sequential(\n",
            "        (0): FeedForwardModule(\n",
            "          (layers): Sequential(\n",
            "            (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "            (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "            (2): SiLU()\n",
            "            (3): Dropout(p=0.1, inplace=False)\n",
            "            (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "            (5): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): MultiHeadAttentionModule(\n",
            "          (layers): Sequential(\n",
            "            (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "            (1): TransformerEncoder(\n",
            "              (layers): ModuleList(\n",
            "                (0-15): 16 x TransformerEncoderLayer(\n",
            "                  (self_attn): MultiheadAttention(\n",
            "                    (out_proj): NonDynamicallyQuantizableLinear(in_features=144, out_features=144, bias=True)\n",
            "                  )\n",
            "                  (linear1): Linear(in_features=144, out_features=256, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  (linear2): Linear(in_features=256, out_features=144, bias=True)\n",
            "                  (norm1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "                  (norm2): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "                  (dropout1): Dropout(p=0.1, inplace=False)\n",
            "                  (dropout2): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (2): ConvolutionModule(\n",
            "          (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (layers): Sequential(\n",
            "            (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "            (1): GLU(dim=1)\n",
            "            (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "            (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (4): SiLU()\n",
            "            (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "            (6): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): FeedForwardModule(\n",
            "          (layers): Sequential(\n",
            "            (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "            (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "            (2): SiLU()\n",
            "            (3): Dropout(p=0.1, inplace=False)\n",
            "            (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "            (5): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (3): ConformerBlock(\n",
            "      (layers): Sequential(\n",
            "        (0): FeedForwardModule(\n",
            "          (layers): Sequential(\n",
            "            (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "            (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "            (2): SiLU()\n",
            "            (3): Dropout(p=0.1, inplace=False)\n",
            "            (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "            (5): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): MultiHeadAttentionModule(\n",
            "          (layers): Sequential(\n",
            "            (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "            (1): TransformerEncoder(\n",
            "              (layers): ModuleList(\n",
            "                (0-15): 16 x TransformerEncoderLayer(\n",
            "                  (self_attn): MultiheadAttention(\n",
            "                    (out_proj): NonDynamicallyQuantizableLinear(in_features=144, out_features=144, bias=True)\n",
            "                  )\n",
            "                  (linear1): Linear(in_features=144, out_features=256, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  (linear2): Linear(in_features=256, out_features=144, bias=True)\n",
            "                  (norm1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "                  (norm2): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "                  (dropout1): Dropout(p=0.1, inplace=False)\n",
            "                  (dropout2): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (2): ConvolutionModule(\n",
            "          (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (layers): Sequential(\n",
            "            (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "            (1): GLU(dim=1)\n",
            "            (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "            (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (4): SiLU()\n",
            "            (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "            (6): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): FeedForwardModule(\n",
            "          (layers): Sequential(\n",
            "            (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "            (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "            (2): SiLU()\n",
            "            (3): Dropout(p=0.1, inplace=False)\n",
            "            (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "            (5): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (layers): Sequential(\n",
            "      (0): Linear(in_features=144, out_features=320, bias=True)\n",
            "      (1): ReLU()\n",
            "      (2): Linear(in_features=320, out_features=28, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (log_softmax): LogSoftmax(dim=-1)\n",
            ")\n",
            "Loading model weights from: model/model_best.pth ...\n",
            "test-clean: 100% 328/328 [59:23<00:00, 10.87s/it]\n",
            "test-other: 100% 368/368 [58:42<00:00,  9.57s/it]\n",
            "    test-clean_CER_(Argmax): 0.1344306800499194\n",
            "    test-clean_WER_(Argmax): 0.4012853578322963\n",
            "    test-clean_CER_(BeamSearch): 0.13385946017372752\n",
            "    test-clean_WER_(BeamSearch): 0.39967944386248255\n",
            "    test-other_CER_(Argmax): 0.3069152766804879\n",
            "    test-other_WER_(Argmax): 0.6717934660576239\n",
            "    test-other_CER_(BeamSearch): 0.3058145674314093\n",
            "    test-other_WER_(BeamSearch): 0.6704460640080676\n"
          ]
        }
      ]
    }
  ]
}