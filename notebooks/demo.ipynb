{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aKzi5VDjPTnU"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67bKw8NRPiBG"
      },
      "source": [
        "Clone repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UO2hE7tANyfi",
        "outputId": "215d6256-a4e8-4654-a5ab-248893766c9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'asr'...\n",
            "remote: Enumerating objects: 366, done.\u001b[K\n",
            "remote: Counting objects: 100% (366/366), done.\u001b[K\n",
            "remote: Compressing objects: 100% (222/222), done.\u001b[K\n",
            "remote: Total 366 (delta 185), reused 297 (delta 116), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (366/366), 383.82 KiB | 31.98 MiB/s, done.\n",
            "Resolving deltas: 100% (185/185), done.\n",
            "/content/asr\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/7embl4/asr.git asr\n",
        "%cd asr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlX9QYPYPkWI"
      },
      "source": [
        "Install all required libs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-pyXr0P1PN7D"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdqV9Q0wQyRr"
      },
      "source": [
        "Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DdJKGQ1ZQzxW"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli download artem1085715/conformer-small --local-dir 'model/'\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n63H5kX9RJBq"
      },
      "source": [
        "Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIPmzZFsWQOv",
        "outputId": "ee7e5dc6-e4a6-43ea-cc37-65ca9f99e5b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1kyESoLRy5oCGE7y_vFUbNZxQenxfHigd\n",
            "To: /content/asr/test_data.zip\n",
            "\r  0% 0.00/746k [00:00<?, ?B/s]\r100% 746k/746k [00:00<00:00, 87.3MB/s]\n",
            "Archive:  test_data.zip\n",
            "   creating: ./test_data/\n",
            "   creating: ./test_data/audio/\n",
            "  inflating: ./test_data/audio/84-121550-0000.flac  \n",
            "  inflating: ./test_data/audio/84-121550-0001.flac  \n",
            "  inflating: ./test_data/audio/84-121550-0002.flac  \n",
            "  inflating: ./test_data/audio/84-121550-0003.flac  \n",
            "  inflating: ./test_data/audio/84-121550-0004.flac  \n",
            "   creating: ./test_data/transcriptions/\n",
            "  inflating: ./test_data/transcriptions/84-121550-0000.txt  \n",
            "  inflating: ./test_data/transcriptions/84-121550-0001.txt  \n",
            "  inflating: ./test_data/transcriptions/84-121550-0002.txt  \n",
            "  inflating: ./test_data/transcriptions/84-121550-0003.txt  \n",
            "  inflating: ./test_data/transcriptions/84-121550-0004.txt  \n"
          ]
        }
      ],
      "source": [
        "!gdown 1kyESoLRy5oCGE7y_vFUbNZxQenxfHigd\n",
        "!unzip test_data.zip -d .\n",
        "!rm test_data.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8fccYtcRpJw"
      },
      "source": [
        "Run inference. You have to provide:\n",
        "- path to model you use\n",
        "- config for a custom datasets\n",
        "- paths to audio and it's transcriptions\n",
        "- *optional:* if your dataset has less elements than default batch size of dataloader (8 by default), you should specify it in hydra parameters too\n",
        "\n",
        "Note: Your custom dataset should follow this structure as shown below\n",
        "```bash\n",
        "dataset_name\n",
        "|-- audio\n",
        "|   |-- audio1.wav  # might be .flac\n",
        "|   |-- audio2.wav\n",
        "|   |-- ...\n",
        "|-- transcriptions\n",
        "|   |-- transcription1.txt\n",
        "|   |-- transcription2.txt\n",
        "|   |-- ...\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GW4ttmTERr7U",
        "outputId": "44f868fe-c427-411d-ae1e-e80a910c2e61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CTCModel(\n",
            "  (subsampling): Subsampling(\n",
            "    (conv): Conv1d(80, 80, kernel_size=(2,), stride=(2,))\n",
            "    (linear): Linear(in_features=80, out_features=144, bias=True)\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (conf_blocks): Sequential(\n",
            "    (0): ConformerBlock(\n",
            "      (layers): Sequential(\n",
            "        (0): FeedForwardModule(\n",
            "          (layers): Sequential(\n",
            "            (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "            (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "            (2): SiLU()\n",
            "            (3): Dropout(p=0.1, inplace=False)\n",
            "            (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "            (5): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): MultiHeadAttentionModule(\n",
            "          (layers): Sequential(\n",
            "            (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "            (1): TransformerEncoder(\n",
            "              (layers): ModuleList(\n",
            "                (0-15): 16 x TransformerEncoderLayer(\n",
            "                  (self_attn): MultiheadAttention(\n",
            "                    (out_proj): NonDynamicallyQuantizableLinear(in_features=144, out_features=144, bias=True)\n",
            "                  )\n",
            "                  (linear1): Linear(in_features=144, out_features=256, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  (linear2): Linear(in_features=256, out_features=144, bias=True)\n",
            "                  (norm1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "                  (norm2): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "                  (dropout1): Dropout(p=0.1, inplace=False)\n",
            "                  (dropout2): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (2): ConvolutionModule(\n",
            "          (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (layers): Sequential(\n",
            "            (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "            (1): GLU(dim=1)\n",
            "            (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "            (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (4): SiLU()\n",
            "            (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "            (6): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): FeedForwardModule(\n",
            "          (layers): Sequential(\n",
            "            (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "            (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "            (2): SiLU()\n",
            "            (3): Dropout(p=0.1, inplace=False)\n",
            "            (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "            (5): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (1): ConformerBlock(\n",
            "      (layers): Sequential(\n",
            "        (0): FeedForwardModule(\n",
            "          (layers): Sequential(\n",
            "            (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "            (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "            (2): SiLU()\n",
            "            (3): Dropout(p=0.1, inplace=False)\n",
            "            (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "            (5): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): MultiHeadAttentionModule(\n",
            "          (layers): Sequential(\n",
            "            (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "            (1): TransformerEncoder(\n",
            "              (layers): ModuleList(\n",
            "                (0-15): 16 x TransformerEncoderLayer(\n",
            "                  (self_attn): MultiheadAttention(\n",
            "                    (out_proj): NonDynamicallyQuantizableLinear(in_features=144, out_features=144, bias=True)\n",
            "                  )\n",
            "                  (linear1): Linear(in_features=144, out_features=256, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  (linear2): Linear(in_features=256, out_features=144, bias=True)\n",
            "                  (norm1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "                  (norm2): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "                  (dropout1): Dropout(p=0.1, inplace=False)\n",
            "                  (dropout2): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (2): ConvolutionModule(\n",
            "          (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (layers): Sequential(\n",
            "            (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "            (1): GLU(dim=1)\n",
            "            (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "            (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (4): SiLU()\n",
            "            (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "            (6): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): FeedForwardModule(\n",
            "          (layers): Sequential(\n",
            "            (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "            (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "            (2): SiLU()\n",
            "            (3): Dropout(p=0.1, inplace=False)\n",
            "            (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "            (5): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (2): ConformerBlock(\n",
            "      (layers): Sequential(\n",
            "        (0): FeedForwardModule(\n",
            "          (layers): Sequential(\n",
            "            (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "            (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "            (2): SiLU()\n",
            "            (3): Dropout(p=0.1, inplace=False)\n",
            "            (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "            (5): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): MultiHeadAttentionModule(\n",
            "          (layers): Sequential(\n",
            "            (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "            (1): TransformerEncoder(\n",
            "              (layers): ModuleList(\n",
            "                (0-15): 16 x TransformerEncoderLayer(\n",
            "                  (self_attn): MultiheadAttention(\n",
            "                    (out_proj): NonDynamicallyQuantizableLinear(in_features=144, out_features=144, bias=True)\n",
            "                  )\n",
            "                  (linear1): Linear(in_features=144, out_features=256, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  (linear2): Linear(in_features=256, out_features=144, bias=True)\n",
            "                  (norm1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "                  (norm2): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "                  (dropout1): Dropout(p=0.1, inplace=False)\n",
            "                  (dropout2): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (2): ConvolutionModule(\n",
            "          (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (layers): Sequential(\n",
            "            (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "            (1): GLU(dim=1)\n",
            "            (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "            (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (4): SiLU()\n",
            "            (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "            (6): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): FeedForwardModule(\n",
            "          (layers): Sequential(\n",
            "            (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "            (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "            (2): SiLU()\n",
            "            (3): Dropout(p=0.1, inplace=False)\n",
            "            (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "            (5): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (3): ConformerBlock(\n",
            "      (layers): Sequential(\n",
            "        (0): FeedForwardModule(\n",
            "          (layers): Sequential(\n",
            "            (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "            (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "            (2): SiLU()\n",
            "            (3): Dropout(p=0.1, inplace=False)\n",
            "            (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "            (5): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): MultiHeadAttentionModule(\n",
            "          (layers): Sequential(\n",
            "            (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "            (1): TransformerEncoder(\n",
            "              (layers): ModuleList(\n",
            "                (0-15): 16 x TransformerEncoderLayer(\n",
            "                  (self_attn): MultiheadAttention(\n",
            "                    (out_proj): NonDynamicallyQuantizableLinear(in_features=144, out_features=144, bias=True)\n",
            "                  )\n",
            "                  (linear1): Linear(in_features=144, out_features=256, bias=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  (linear2): Linear(in_features=256, out_features=144, bias=True)\n",
            "                  (norm1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "                  (norm2): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "                  (dropout1): Dropout(p=0.1, inplace=False)\n",
            "                  (dropout2): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (2): ConvolutionModule(\n",
            "          (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "          (layers): Sequential(\n",
            "            (0): Conv1d(144, 288, kernel_size=(1,), stride=(1,))\n",
            "            (1): GLU(dim=1)\n",
            "            (2): Conv1d(144, 144, kernel_size=(31,), stride=(1,), padding=(15,), groups=144)\n",
            "            (3): BatchNorm1d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            (4): SiLU()\n",
            "            (5): Conv1d(144, 144, kernel_size=(1,), stride=(1,))\n",
            "            (6): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): FeedForwardModule(\n",
            "          (layers): Sequential(\n",
            "            (0): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "            (1): Linear(in_features=144, out_features=576, bias=True)\n",
            "            (2): SiLU()\n",
            "            (3): Dropout(p=0.1, inplace=False)\n",
            "            (4): Linear(in_features=576, out_features=144, bias=True)\n",
            "            (5): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layer_norm): LayerNorm((144,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (layers): Sequential(\n",
            "      (0): Linear(in_features=144, out_features=320, bias=True)\n",
            "      (1): ReLU()\n",
            "      (2): Linear(in_features=320, out_features=28, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (log_softmax): LogSoftmax(dim=-1)\n",
            ")\n",
            "Loading model weights from: model/model_best.pth ...\n",
            "test: 100% 1/1 [00:01<00:00,  1.14s/it]\n",
            "    test_CER_(Argmax): 0.08491907979694456\n",
            "    test_WER_(Argmax): 0.2889834368530021\n",
            "    test_CER_(BeamSearch): 0.0777318530276046\n",
            "    test_WER_(BeamSearch): 0.2789648033126294\n"
          ]
        }
      ],
      "source": [
        "!python inference.py \\\n",
        "        inferencer.from_pretrained=\"model/model_best.pth\" \\\n",
        "        dataloader.batch_size=5 \\\n",
        "        datasets=\"custom\" \\\n",
        "        datasets.test.audio_dir=\"test_data/audio\" \\\n",
        "        datasets.test.transcription_dir=\"test_data/transcriptions\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}